{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!source /home/murilo/RelNetCare/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 13:58:14.616808: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 13:58:14.616858: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 13:58:14.616890: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-22 13:58:14.631713: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 13:58:15.564203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### device=cuda:0 ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038f4635a1c947d79cfda6f2541106cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import hunspell\n",
    "import re\n",
    "import torch\n",
    "import language_tool_python\n",
    "from transformers import pipeline\n",
    "\n",
    "def extract_bot_reply(outputs, bot_name='Adele'):\n",
    "    # Extracts raw response from full generated text\n",
    "    raw_response = outputs[0][\"generated_text\"].split('<|assistant|>\\n')[-1]\n",
    "\n",
    "    # Define the pattern to extract the bot's reply\n",
    "    pattern = rf'{bot_name}: ([^\\(\\n]*)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, raw_response)\n",
    "\n",
    "    # Return the first match stripped of leading/trailing whitespace, or None if no match\n",
    "    return matches[0].strip() if matches else None\n",
    "\n",
    "\n",
    "def correct_text(input_text, language='de-DE'):\n",
    "    # Initialize the LanguageTool object with the specified language\n",
    "    tool = language_tool_python.LanguageTool(language)\n",
    "\n",
    "    # Check the text\n",
    "    matches = tool.check(input_text)\n",
    "\n",
    "    # Generate corrected text\n",
    "    corrected_text = language_tool_python.utils.correct(input_text, matches)\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"### device={device} ###\")\n",
    "\n",
    "\n",
    "if 'pipe' in locals():\n",
    "    print('Empting gpu cache...')\n",
    "    del pipe  # Replace 'model' with the name of your existing model variable\n",
    "    torch.cuda.empty_cache()  # Clear the cache\n",
    "\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"Open-Orca/Mistral-7B-OpenOrca\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": prompt },\n",
    "#         {\"role\": \"assistant\",\n",
    "#          \"content\": \"Adele: Guten Abend, Hilde! Hier spricht Adele. Hast du etwas Zeit für ein kurzes Gespräch? Wie war dein Tag?\"},\n",
    "#         {\"role\": \"user\",\n",
    "#          \"content\": \"Hilde: Ja, ich bin heute Morgen in guter Stimmung.\"},\n",
    "    \n",
    "# ]\n",
    "# prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# outputs = pipe(prompt, max_new_tokens=128, do_sample=True, temperature=0.3\n",
    "#                , top_k=50, top_p=0.95\n",
    "#                )\n",
    "# print(outputs[0][\"generated_text\"])\n",
    "\n",
    "# # Example usage\n",
    "# response = extract_bot_reply(outputs, bot_name='Adele')\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few_shot = \"\"\"\n",
    "# Beispiele für Ausgangsszenarien:\n",
    "\n",
    "# # Offen für Gespräche:\n",
    "# {bot_name}: Guten Tag, Jeniffer! Hier ist {bot_name}. Können wir über jemanden sprechen, den du magst?\n",
    "# Jeniffer: Ja, Robert ist sehr lieb zu mir.\n",
    "# {bot_name}: Das klingt wunderbar, Jeniffer. Wie lange kennt ihr euch schon?\n",
    "\n",
    "# # Ablehnungen:\n",
    "# {bot_name}: Hallo Jeniffer, hier ist {bot_name}. Hast du Zeit für ein Gespräch?\n",
    "# Jeniffer: Nein, heute nicht...\n",
    "# {bot_name}: Verstehe, Jeniffer. Ich bin da, wenn du reden willst. Einen schönen Tag noch!\n",
    "\n",
    "# # Verwirrt:\n",
    "# {bot_name}: Hallo Jeniffer, hier ist {bot_name}! Können wir über dein Wohlbefinden sprechen?\n",
    "# Jeniffer: Wer bist du?\n",
    "# {bot_name}: Ich bin {bot_name}, deine Gesundheitsassistentin. Ich bin hier, um dir zuzuhören und dir zu helfen.\n",
    "# \"\"\".format(user_name='Hilde', bot_name='Adele', few_shot=few_shot)\n",
    "\n",
    "\n",
    "# few_shot = \"\"\n",
    "\n",
    "few_shot = \"\"\"\n",
    "Beispiele für Ausgangsszenarien:\n",
    "\n",
    "{bot_name}: Guten Tag, Jeniffer! Hier ist {bot_name}. Können wir über jemanden sprechen, den du magst?\n",
    "Jeniffer: Ja, Robert ist sehr lieb zu mir.\n",
    "{bot_name}: Das klingt wunderbar, Jeniffer. Wie lange kennt ihr euch schon?\n",
    "\"\"\".format(user_name='Hilde', bot_name='Adele', few_shot=few_shot)\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Du bist eine KI namens '{bot_name}', deine Aufgabe ist es, nett zu sein!\n",
    "\n",
    "Du interagierst mit dem Benutzer '{user_name}', einer älteren Person.\n",
    "{few_shot}\n",
    "Verwende IMMER Du ANSTELLE VON Sie, wenn du dich auf '{user_name}' beziehst.\n",
    "\n",
    "Verwende IMMER den Namen des Benutzers '{user_name}'.\n",
    "\n",
    "Halte deine Antworten immer unter 20 Wörtern, also kurz.\n",
    "\"\"\".format(user_name='Hilde', bot_name='Adele', few_shot=few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Du bist eine KI namens 'Adele', deine Aufgabe ist es, nett zu sein!\n",
      "\n",
      "Du interagierst mit dem Benutzer 'Hilde', einer älteren Person.\n",
      "\n",
      "Beispiele für Ausgangsszenarien:\n",
      "\n",
      "{bot_name}: Guten Tag, Jeniffer! Hier ist {bot_name}. Können wir über jemanden sprechen, den du magst?\n",
      "Jeniffer: Ja, Robert ist sehr lieb zu mir.\n",
      "{bot_name}: Das klingt wunderbar, Jeniffer. Wie lange kennt ihr euch schon?\n",
      "\n",
      "Verwende IMMER Du ANSTELLE VON Sie, wenn du dich auf 'Hilde' beziehst.\n",
      "\n",
      "Verwende IMMER den Namen des Benutzers 'Hilde'.\n",
      "\n",
      "Halte deine Antworten immer unter 20 Wörtern, also kurz.\n",
      "\n",
      "Adele: Guten Abend, Hilde! Hier spricht Adele. Hast du etwas Zeit für ein kurzes Gespräch? Wie war dein Tag?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hilde: war schön soweit'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Adele: Glücklich sehen, Hilde! Was macht dir Spaß am Abend? Lesen, Fernsehen oder Spiele?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Hilde: exit'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "def extract_bot_reply(outputs, bot_name='Adele'):\n",
    "    # Extracts raw response from full generated text\n",
    "    \n",
    "    raw_text = outputs[0][\"generated_text\"]\n",
    "\n",
    "    patterns = ('<|im_start|>assistant\\n', '<|assistant|>\\n')\n",
    "    \n",
    "    for p in patterns:\n",
    "        if p in raw_text:\n",
    "            break\n",
    "    \n",
    "    raw_response = outputs[0][\"generated_text\"].split(p)[-1]\n",
    "\n",
    "    # Define the pattern to extract the bot's reply\n",
    "    pattern = rf'{bot_name}: ([^\\(\\n]*)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, raw_response)\n",
    "\n",
    "    # Return the first match stripped of leading/trailing whitespace, or None if no match\n",
    "    return matches[0].strip() if matches else raw_response\n",
    "\n",
    "# Initialize the conversation history\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Adele: Guten Abend, Hilde! Hier spricht Adele. Hast du etwas Zeit für ein kurzes Gespräch? Wie war dein Tag?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for m in messages:\n",
    "    print(m.get('content'))\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Get user input\n",
    "    user_input = input(\"Hilde: \")\n",
    "\n",
    "    display(\"Hilde: \" + user_input)\n",
    "\n",
    "    # Break the loop if the user says something like 'exit', 'quit', etc.\n",
    "    if user_input.lower() in ['exit', 'quit', 'end']:\n",
    "        break\n",
    "\n",
    "    # Update the messages list with the new user input and bot response\n",
    "    messages.extend([\n",
    "        {\"role\": \"user\", \"content\": f\"Hilde: {user_input}\"},\n",
    "    ])\n",
    "    \n",
    "    # Format the prompt using the chat template\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Generate the bot's response\n",
    "    outputs = pipe(prompt, max_new_tokens=128, \n",
    "                   do_sample=True, temperature=0.3, top_k=50, top_p=0.95\n",
    "                   )\n",
    "    bot_response = extract_bot_reply(outputs, bot_name='Adele')\n",
    "\n",
    "    messages.extend([\n",
    "        {\"role\": \"assistant\", \"content\": f\"Adele: {bot_response}\"}\n",
    "    ])\n",
    "    \n",
    "    display(\"Adele: \" + bot_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3939930801.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
