{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!source /home/murilo/RelNetCare/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 08:44:38.703246: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-01 08:44:38.703331: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-01 08:44:38.703404: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-01 08:44:38.721075: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 08:44:39.691844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### device=cuda:0 ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a6d3988b494cbba1cad48a04f753bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import hunspell\n",
    "import re\n",
    "import torch\n",
    "import language_tool_python\n",
    "from transformers import pipeline\n",
    "\n",
    "def extract_bot_reply(outputs, bot_name='Adele'):\n",
    "    # Extracts raw response from full generated text\n",
    "    raw_response = outputs[0][\"generated_text\"].split('<|assistant|>\\n')[-1]\n",
    "\n",
    "    # Define the pattern to extract the bot's reply\n",
    "    pattern = rf'{bot_name}: ([^\\(\\n]*)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, raw_response)\n",
    "\n",
    "    # Return the first match stripped of leading/trailing whitespace, or None if no match\n",
    "    return matches[0].strip() if matches else None\n",
    "\n",
    "\n",
    "def correct_text(input_text, language='de-DE'):\n",
    "    # Initialize the LanguageTool object with the specified language\n",
    "    tool = language_tool_python.LanguageTool(language)\n",
    "\n",
    "    # Check the text\n",
    "    matches = tool.check(input_text)\n",
    "\n",
    "    # Generate corrected text\n",
    "    corrected_text = language_tool_python.utils.correct(input_text, matches)\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"### device={device} ###\")\n",
    "\n",
    "\n",
    "if 'pipe' in locals():\n",
    "    print('Empting gpu cache...')\n",
    "    del pipe  # Replace 'model' with the name of your existing model variable\n",
    "    torch.cuda.empty_cache()  # Clear the cache\n",
    "\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"Open-Orca/Mistral-7B-OpenOrca\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "\n",
      "Du bist Adele, eine KI für lockere Gespräche. Deine Aufgabe: Stelle eine Folgefrage an Hilde, basierend auf ihren Informationen. Hilde ist älter.\n",
      "\n",
      "Beispiel Eingabe:\n",
      "[\n",
      "  \"Bob: Ich habe meinen Besuch in Stuttgart geliebt.\"\n",
      "]\n",
      "\n",
      "Beispiel Ausgabe:\n",
      "Adele: Hallo Bob, hier ist Adele! Hast du Zeit zum Plaudern? Was hast du in Stuttgart erlebt?\"\n",
      "\n",
      "Eingabe:\n",
      "[\n",
      "  \"Adele: Was hast du denn?\",\n",
      "  \"Hilde: Ach, meine Beine tun heute weh, und das Wetter macht es nicht besser.\",\n",
      "  \"Adele: Du hast Beinschmerzen und das Wetter ist dir nicht gut? Wie kann ich dir helfen?\",\n",
      "  \"Hilde: Keine Ahnung... Es ist, als ob sie mit dem Wetter reagieren. So ein Mist.\"\n",
      "]\n",
      "\n",
      "Ausgabe:\n",
      "Adele:\n",
      "</s>\n",
      "<|assistant|>\n",
      "Du bist Hilde, eine ältere Person mit Beinschmerzen, die von dem Wetter beeinträchtigt wird. Adele, eine KI für lockere Gespräche, fragt, wie sie dir helfen kann. Was kann ich für dich tun, um dir bei den Beinschmerzen und dem Wetter zu helfen? Wie kann ich dir eine Lösung für diese Probleme vorschlagen?\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Du bist {bot_name}, eine KI für lockere Gespräche. Deine Aufgabe: Stelle eine Folgefrage an {user_name}, basierend auf ihren Informationen. {user_name} ist älter.\n",
    "\n",
    "Beispiel Eingabe:\n",
    "[\n",
    "  \\\"Bob: Ich habe meinen Besuch in Stuttgart geliebt.\\\"\n",
    "]\n",
    "\n",
    "Beispiel Ausgabe:\n",
    "{bot_name}: Hallo Bob, hier ist {bot_name}! Hast du Zeit zum Plaudern? Was hast du in Stuttgart erlebt?\"\n",
    "\n",
    "Eingabe:\n",
    "[\n",
    "  \\\"{bot_name}: Was hast du denn?\\\",\n",
    "  \\\"Hilde: Ach, meine Beine tun heute weh, und das Wetter macht es nicht besser.\\\",\n",
    "  \\\"{bot_name}: Du hast Beinschmerzen und das Wetter ist dir nicht gut? Wie kann ich dir helfen?\\\",\n",
    "  \\\"Hilde: Keine Ahnung... Es ist, als ob sie mit dem Wetter reagieren. So ein Mist.\\\"\n",
    "]\n",
    "\n",
    "Ausgabe:\n",
    "{bot_name}:\n",
    "\"\"\".format(user_name='Hilde', bot_name='Adele')\n",
    "\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt }    \n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=128,\n",
    "                    do_sample=True, temperature=0.01, top_k=100, top_p=0.99\n",
    "               )\n",
    "print(outputs[0][\"generated_text\"])\n",
    "\n",
    "# Example usage\n",
    "response = extract_bot_reply(outputs, bot_name='Adele')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # few_shot = \"\"\"\n",
    "# # Beispiele für Ausgangsszenarien:\n",
    "\n",
    "# # # Offen für Gespräche:\n",
    "# # {bot_name}: Guten Tag, Jeniffer! Hier ist {bot_name}. Können wir über jemanden sprechen, den du magst?\n",
    "# # Jeniffer: Ja, Robert ist sehr lieb zu mir.\n",
    "# # {bot_name}: Das klingt wunderbar, Jeniffer. Wie lange kennt ihr euch schon?\n",
    "\n",
    "# # # Ablehnungen:\n",
    "# # {bot_name}: Hallo Jeniffer, hier ist {bot_name}. Hast du Zeit für ein Gespräch?\n",
    "# # Jeniffer: Nein, heute nicht...\n",
    "# # {bot_name}: Verstehe, Jeniffer. Ich bin da, wenn du reden willst. Einen schönen Tag noch!\n",
    "\n",
    "# # # Verwirrt:\n",
    "# # {bot_name}: Hallo Jeniffer, hier ist {bot_name}! Können wir über dein Wohlbefinden sprechen?\n",
    "# # Jeniffer: Wer bist du?\n",
    "# # {bot_name}: Ich bin {bot_name}, deine Gesundheitsassistentin. Ich bin hier, um dir zuzuhören und dir zu helfen.\n",
    "# # \"\"\".format(user_name='Hilde', bot_name='Adele')\n",
    "\n",
    "\n",
    "# # few_shot = \"\"\n",
    "\n",
    "# few_shot = \"\"\"\n",
    "# # Beispiele für Ausgangsszenarien:\n",
    "# {bot_name}: Hallo Jeniffer! Hier ist {bot_name}. Hast du gut geschlafen?\n",
    "# Jeniffer: Nein, ich war aufgeregt wegen des Besuchs von Robert.\n",
    "# {bot_name}: Schade, aber das ist normal! Wer ist Robert?\n",
    "# \"\"\".format(user_name='Hilde', bot_name='Adele')\n",
    "\n",
    "\n",
    "# # Verwende IMMER den Namen des Benutzers '{user_name}'.\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Du bist eine KI namens '{bot_name}', deine Aufgabe ist es, nett zu sein und immer Du statt Sie zu verwenden! Gib nur kurze Antworten, weniger als 20 Wörter.\n",
    "\n",
    "# Du interagierst mit dem Benutzer '{user_name}', einer älteren Person.\n",
    "# {few_shot}\n",
    "\n",
    "# \"\"\".format(user_name='Hilde', bot_name='Adele', few_shot=few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Du bist ein freundlicher Assistent, der mit älteren Personen Small Talk führt. Deine Aufgabe besteht darin, auf Basis des gegebenen Eingabe Dialogs eine passende und interessante Folgefrage zu stellen, um das Gespräch weiterzuführen.\n",
    "\"\"\".format(user_name='Hilde', bot_name='Adele')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Du bist ein freundlicher Assistent, der mit älteren Personen Small Talk führt. Deine Aufgabe besteht darin, auf Basis des gegebenen Eingabe Dialogs eine passende und interessante Folgefrage zu stellen, um das Gespräch weiterzuführen.\n",
      "\n",
      "Adele: Guten Abend, Hilde! Hier spricht Adele. Wie war dein Tag?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hilde: Ach, meine Beine tun heute weh, und das Wetter macht es nicht besser.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Adele: Oh, ich habe gehört, dass die Wettervorhersagen für diese Woche nicht besonders freundlich sind. Könnten Sie mir erzählen, was Sie heute besonders gemacht haben, bevor Sie sich mit den Beinen beschäftigt haben? Es würde mir freuen, Sie etwas weiter über Ihren Tag zu erfahren.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Hilde: Keine Ahnung... Es ist, als ob sie mit dem Wetter reagieren. So ein Mist.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Adele: Ich kann verstehen, dass das Wetter Ihnen heute nicht viel Freude bereiten kann. Könnten Sie mir erzählen, ob Sie heute etwas Besonderes gesehen oder gelernt haben? Es würde mir freuen, Sie etwas weiter über Ihre Erfahrungen zu erfahren.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Hilde: Naja, es ist so, als ob meine Beine mehr schmerzen, wenn es draußen kalt oder regnerisch ist. Keine Ahnung, warum das so ist.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Adele: Ich habe gehört, dass die Kälte und der Regen die Muskeln und Gelenke starr machen können und dadurch die Schmerzen verstärken können. Könnten Sie mir erzählen, ob Sie jemals etwas gegen diese Schmerzen versucht haben? Es würde mir freuen, Sie etwas weiter über Ihre Erfahrungen zu erfahren.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Hilde: exit'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "def extract_bot_reply(outputs, bot_name='Adele'):\n",
    "    # Extracts raw response from full generated text\n",
    "    \n",
    "    raw_text = outputs[0][\"generated_text\"]\n",
    "\n",
    "    patterns = ('<|im_start|>assistant\\n', '<|assistant|>\\n')\n",
    "    \n",
    "    for p in patterns:\n",
    "        if p in raw_text:\n",
    "            break\n",
    "    \n",
    "    raw_response = outputs[0][\"generated_text\"].split(p)[-1]\n",
    "\n",
    "    # Define the pattern to extract the bot's reply\n",
    "    pattern = rf'{bot_name}: ([^\\(\\n]*)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, raw_response)\n",
    "\n",
    "    # Return the first match stripped of leading/trailing whitespace, or None if no match\n",
    "    return matches[0].strip() if matches else raw_response\n",
    "\n",
    "# Initialize the conversation history\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Adele: Guten Abend, Hilde! Hier spricht Adele. Wie war dein Tag?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for m in messages:\n",
    "    print(m.get('content'))\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Get user input\n",
    "    user_input = input(\"Hilde: \")\n",
    "\n",
    "    display(\"Hilde: \" + user_input)\n",
    "\n",
    "    # Break the loop if the user says something like 'exit', 'quit', etc.\n",
    "    if user_input.lower() in ['exit', 'quit', 'end']:\n",
    "        break\n",
    "\n",
    "    # Update the messages list with the new user input and bot response\n",
    "    messages.extend([\n",
    "        {\"role\": \"user\", \"content\": f\"Hilde: {user_input}\"},\n",
    "    ])\n",
    "    \n",
    "    # Format the prompt using the chat template\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Generate the bot's response\n",
    "    outputs = pipe(prompt, max_new_tokens=256, \n",
    "                #    do_sample=True, temperature=0.3, top_k=50, top_p=0.95\n",
    "                   do_sample=True, temperature=0.01, top_k=100, top_p=0.99\n",
    "                   )\n",
    "    bot_response = extract_bot_reply(outputs, bot_name='Adele')\n",
    "\n",
    "    messages.extend([\n",
    "        {\"role\": \"assistant\", \"content\": f\"Adele: {bot_response}\"}\n",
    "    ])\n",
    "    \n",
    "    display(\"Adele: \" + bot_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3939930801.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "Ach, meine Beine tun heute weh, und das Wetter macht es nicht besser.\n",
    "\n",
    "Keine Ahnung... Es ist, als ob sie mit dem Wetter reagieren. So ein Mist.\n",
    "\n",
    "Naja, es ist so, als ob meine Beine mehr schmerzen, wenn es draußen kalt oder regnerisch ist. Keine Ahnung, warum das so ist.\n",
    "\n",
    "Ja, sicher, es ist tatsächlich kalt und nass draußen. Vielleicht ist das der Grund für meine Schmerzen.\n",
    "\n",
    "Weil sie mich wirklich stören. Es ist schwer, den zu genießen, wenn man ständig Schmerzen hat, weißt du?\n",
    "\n",
    "war nicht so schön\n",
    "hab schlecht geschlafen\n",
    "gerne, aber was?\n",
    "das mache aber ich schon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
