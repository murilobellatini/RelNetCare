{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!source /home/murilo/RelNetCare/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm  # <--- Use notebook version for Jupyter\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "exp_group = \"DialogRESredfmBARTSummarizeRelations\"  # Change this for each run\n",
    "data_folder = \"/home/murilo/RelNetCare/data/processed/dialog-re-llama-11cls-rebalPairs-rwrtKeys-instrC-mxTrnCp3-skpTps-prepBART\"\n",
    "freeze_encoder = False\n",
    "model_name = 'facebook/bart-base'\n",
    "\n",
    "if model_name == 'facebook/bart-base':\n",
    "    train_batch_size = 24 # was 32\n",
    "    eval_batch_size = 48 # was 64\n",
    "elif model_name == 'facebook/bart-large':\n",
    "    train_batch_size = 6\n",
    "    eval_batch_size = 12\n",
    "else:\n",
    "    raise Exception(\"Model without batch size match\")\n",
    "\n",
    "# Initialize Weights and Biases with more args\n",
    "args_dict = {\n",
    "    \"per_device_train_batch_size\": train_batch_size,\n",
    "    \"per_device_eval_batch_size\": eval_batch_size,\n",
    "    \"num_train_epochs\": 5,\n",
    "    'learning_rate': 2e-5,  # good starting point: 2e-5 (min acceptable for large 2.5e-7)\n",
    "    \"exp_group\": exp_group,\n",
    "    \"data_stem\": data_folder.split('/')[-1],\n",
    "    \"data_folder\": data_folder,\n",
    "    \"freeze_encoder\": freeze_encoder,\n",
    "    'truncation': True,\n",
    "    'max_length': 512,\n",
    "    'model_name':model_name,\n",
    "    'memorization_task': False,\n",
    "    'fp16': False,\n",
    "    'merge_train_dev': False,\n",
    "    'dropout_regularization_proba': None, # default 0.10\n",
    "}\n",
    "args_dict['output_dir'] = f\"/mnt/vdb1/murilo/models/fine-tuned/{args_dict['model_name']}/{args_dict['data_stem']}\"\n",
    "\n",
    "\n",
    "checkpoint = args_dict['model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(args_dict['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 2304\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 841\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 705\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_cap = -1\n",
    "set_data = None\n",
    "dataset_sets = {}\n",
    "dict_sets = {}\n",
    "for set_ in ('train', 'test', 'dev'):\n",
    "\n",
    "    data_path = os.path.join(data_folder, f'{set_}.json')\n",
    "\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "            \n",
    "    # Remap keys and separate into train/test\n",
    "    if args_dict['memorization_task']:\n",
    "        if not set_data:\n",
    "            set_data = [{\"text\": item[\"input\"], \"summary\": item[\"output\"], \"title\": \"\"} for item in data[data_cap:]]\n",
    "    else:\n",
    "        set_data = [{\"text\": item[\"input\"], \"summary\": item[\"output\"], \"title\": \"\"} for item in data]\n",
    "        \n",
    "    # Merge 'train' and 'dev' if the flag is set\n",
    "    if args_dict['merge_train_dev']:\n",
    "        if set_ == 'dev':\n",
    "            dict_sets['train'] = dict_sets['train'] + set_data\n",
    "        else:\n",
    "           dict_sets[set_] = set_data\n",
    "    else:\n",
    "        dict_sets[set_] = set_data\n",
    "        \n",
    "\n",
    "    \n",
    "for set_ in ('train', 'test', 'dev'):\n",
    "    if args_dict['merge_train_dev']:\n",
    "        if set_ == 'dev':\n",
    "            continue\n",
    "    set_data = dict_sets[set_]\n",
    "    dataset_sets[set_] = Dataset.from_dict(\n",
    "        {\"text\": [item[\"text\"] for item in set_data],\n",
    "         \"summary\": [item[\"summary\"] for item in set_data],\n",
    "         \"title\": [item[\"title\"] for item in set_data]}\n",
    "        )\n",
    "    \n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict(dataset_sets)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a533d6bcda5745a08e12eaac1dbdd66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475a7a4ad386475fbe619db83de20a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e927afd6e754c0cb8c69298989d0c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples, model_max_length=args_dict['max_length'], tokenizer_max_length=args_dict['max_length']):\n",
    "    inputs = [doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=model_max_length, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=model_max_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_data = dataset_dict.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=args_dict['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/murilo/miniconda3/envs/llama-lora/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /home/murilo/miniconda3/envs/llama-lora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2023-10-09 13:35:56,768] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murilo/miniconda3/envs/llama-lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model loaded...\n",
      "model.config.encoder_layerdrop= 0.0\n",
      "model.config.decoder_layerdrop= 0.0\n",
      "model.config.dropout= 0.1\n",
      "model.config.attention_dropout= 0.1\n",
      "model.config.activation_dropout= 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'per_device_train_batch_size': 24,\n",
       " 'per_device_eval_batch_size': 48,\n",
       " 'num_train_epochs': 5,\n",
       " 'learning_rate': 2e-05,\n",
       " 'exp_group': 'DialogRESredfmBARTSummarizeRelations',\n",
       " 'data_stem': 'dialog-re-llama-11cls-rebalPairs-rwrtKeys-instrC-mxTrnCp3-skpTps-prepBART',\n",
       " 'data_folder': '/home/murilo/RelNetCare/data/processed/dialog-re-llama-11cls-rebalPairs-rwrtKeys-instrC-mxTrnCp3-skpTps-prepBART',\n",
       " 'freeze_encoder': False,\n",
       " 'truncation': True,\n",
       " 'max_length': 512,\n",
       " 'model_name': 'facebook/bart-base',\n",
       " 'memorization_task': False,\n",
       " 'fp16': False,\n",
       " 'merge_train_dev': False,\n",
       " 'dropout_regularization_proba': None,\n",
       " 'output_dir': '/mnt/vdb1/murilo/models/fine-tuned/facebook/bart-base/dialog-re-llama-11cls-rebalPairs-rwrtKeys-instrC-mxTrnCp3-skpTps-prepBART'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, BartConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "try:\n",
    "    # Release GPU\n",
    "    model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    print(\"No model loaded...\")\n",
    "\n",
    "if args_dict['dropout_regularization_proba']:\n",
    "    config_extreme = BartConfig.from_pretrained(checkpoint,\n",
    "                                                # encoder_layerdrop=0.2,\n",
    "                                                # decoder_layerdrop=0.5,\n",
    "                                                dropout=args_dict['dropout_regularization_proba'],\n",
    "                                                # attention_dropout=0.5,\n",
    "                                                # activation_dropout=0.5\n",
    "                                                )\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, config=config_extreme)\n",
    "\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "if args_dict['freeze_encoder']:\n",
    "    print('freezing encoder!')\n",
    "    for param in model.model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "print(\"model.config.encoder_layerdrop=\",model.config.encoder_layerdrop)\n",
    "print(\"model.config.decoder_layerdrop=\",model.config.decoder_layerdrop)\n",
    "print(\"model.config.dropout=\",model.config.dropout)\n",
    "print(\"model.config.attention_dropout=\",model.config.attention_dropout)\n",
    "print(\"model.config.activation_dropout=\",model.config.activation_dropout)\n",
    "\n",
    "model.to(device)\n",
    "args_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmbellatini\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/murilo/RelNetCare/reports/03_model_training/wandb/run-20231009_133610-ghv2fylu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mbellatini/huggingface/runs/ghv2fylu' target=\"_blank\">young-deluge-235</a></strong> to <a href='https://wandb.ai/mbellatini/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mbellatini/huggingface' target=\"_blank\">https://wandb.ai/mbellatini/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mbellatini/huggingface/runs/ghv2fylu' target=\"_blank\">https://wandb.ai/mbellatini/huggingface/runs/ghv2fylu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murilo/miniconda3/envs/llama-lora/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 03:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.432354</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>19.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>18.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.347971</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>19.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>0.356209</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.462200</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>19.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.327400</td>\n",
       "      <td>0.334293</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>18.357400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=480, training_loss=0.4008096211589873, metrics={'train_runtime': 190.3085, 'train_samples_per_second': 60.533, 'train_steps_per_second': 2.522, 'total_flos': 925279838945280.0, 'train_loss': 0.4008096211589873, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"huggingface\", config=args_dict, reinit=True)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=args_dict['output_dir'],\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=args_dict['learning_rate'],\n",
    "    per_device_train_batch_size=args_dict['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=args_dict['per_device_eval_batch_size'],\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=args_dict['num_train_epochs'],\n",
    "    predict_with_generate=True,\n",
    "    fp16=args_dict['fp16'],\n",
    "    load_best_model_at_end=True,\n",
    "    # max_grad_norm=0.1,\n",
    "    seed=42,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# @todo: set random seed fo training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input= Speaker 1: No, I-I mean your-your old best friend, here. Lily, from high school. Remember? Speaker 2: Oh gosh, Lily, yes. Of course I remember Lily. I... Then you must be? Speaker 1: Phoebe. Phoebe. Phoebe, yeah. She named me after you I guess.\n",
      "ground truth = Speaker 1 is a parent of Lily. Lily is a child of Speaker 1.\n",
      "raw inference= Speaker 1 is a parent of Lily. Lily is a child of Speaker 1.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "idx = -69\n",
    "text = dataset_dict['train'][idx]['text']\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# overfit_accomplished = dataset_dict['train'][0]['summary'] == summary\n",
    "# print(\"overfit_accomplished=\",overfit_accomplished)\n",
    "print(\"input=\", dataset_dict['train'][idx]['text'])\n",
    "print(\"ground truth =\",dataset_dict['train'][idx]['summary'])\n",
    "print(\"raw inference=\",summary )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo: create a custom loss\n",
    "# first inspect this\n",
    "\n",
    "import inspect\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# print(inspect.getsource(Seq2SeqTrainer.training_step))\n",
    "# print(inspect.getsource(Seq2SeqTrainer.compute_loss))\n",
    "# print(inspect.getsource(Seq2SeqTrainer.label_smoother))\n",
    "\n",
    "# # then overwrite the methods (either compute_loss or training_step, or even both)\n",
    "\n",
    "# from transformers import Seq2SeqTrainer\n",
    "\n",
    "# class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "#     def training_step(self, model, inputs):\n",
    "#         outputs = model(**inputs)\n",
    "#         loss = outputs.loss\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         # Add your custom loss here, let's say L1 regularization\n",
    "#         lambda_l1 = 0.01  # regularization coefficient\n",
    "#         l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "#         loss = loss + lambda_l1 * l1_norm\n",
    "\n",
    "#         self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return {\"loss\": loss}\n",
    "\n",
    "# # lastly the trainer\n",
    "\n",
    "# trainer = CustomSeq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_data[\"train\"],\n",
    "#     eval_dataset=tokenized_data[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"stevhliu/my_awesome_billsum_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release GPU\n",
    "model.cpu()\n",
    "del model\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
