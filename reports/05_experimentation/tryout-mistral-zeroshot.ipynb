{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!source /home/murilo/RelNetCare/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 07:46:45.593384: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-12 07:46:45.593454: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-12 07:46:45.593491: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-12 07:46:45.606446: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-12 07:46:46.465743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/murilo/miniconda3/envs/llama-lora/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /home/murilo/miniconda3/envs/llama-lora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n",
      "[2023-10-12 07:46:47,635] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murilo/miniconda3/envs/llama-lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "dialogue_data = [\n",
    "    {\n",
    "        \"id\": \"identity_354_003\",\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": \"Extract entities and relations from the dialogue. Return a Python list of JSON objects, each fitting this schema: {\\\"subject\\\": \\\"<Entity>\\\", \\\"relation\\\": \\\"<acquaintance/children/other_family/parents/pet/place_of_residence/residents_of_place/siblings/spouse/visited_place/visitors_of_place>\\\", \\\"object\\\": \\\"<Related Entity>\\\"}. No additional text or explanations. Return an empty list if no relevant entities or relations are found. Stick to the provided relations. You are like an API, you don't speak you only return JSON objects.\\nDialogue: \\n[\\n \\\"Speaker 2: Oh no! Why?\\\",\\n \\\"Speaker 1: 'Cause Carol's a lesbian. And, and I'm not one. And apparently it's not a mix and match situation.\\\",\\n \\\"Speaker 2: Oh my God! I don't believe it! Oh, you poor bunny.\\\"\\n]\"\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": \"[\\n {\\n  \\\"subject\\\": \\\"Speaker 1\\\",\\n  \\\"relation\\\": \\\"spouse\\\",\\n  \\\"object\\\": \\\"Carol\\\"\\n },\\n {\\n  \\\"subject\\\": \\\"Carol\\\",\\n  \\\"relation\\\": \\\"spouse\\\",\\n  \\\"object\\\": \\\"Speaker 1\\\"\\n }\\n]\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = dialogue_data[0]['conversations'][0]['value']\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an API that extracts relations from a dialogue prompt in a json list format.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "# prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "# print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extract entities and relations from the dialogue. Return a Python list of JSON objects, each fitting this schema: {\"subject\": \"<Entity>\", \"relation\": \"<acquaintance/children/other_family/parents/pet/place_of_residence/residents_of_place/siblings/spouse/visited_place/visitors_of_place>\", \"object\": \"<Related Entity>\"}. No additional text or explanations. Return an empty list if no relevant entities or relations are found. Stick to the provided relations. You are like an API, you don\\'t speak you only return JSON objects.\\nDialogue: \\n[\\n \"Speaker 2: Oh no! Why?\",\\n \"Speaker 1: \\'Cause Carol\\'s a lesbian. And, and I\\'m not one. And apparently it\\'s not a mix and match situation.\",\\n \"Speaker 2: Oh my God! I don\\'t believe it! Oh, you poor bunny.\"\\n]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
