{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from src.paths import LOCAL_MODELS_PATH\n",
    "from src.processing.text_preprocessing import DialogueEnricher\n",
    "\n",
    "from src.infering import EntityRelationInferer\n",
    "from src.paths import LOCAL_MODELS_PATH, LOCAL_PROCESSED_DATA_PATH, LOCAL_RAW_DATA_PATH\n",
    "\n",
    "from src.infering import EntityExtractor\n",
    "\n",
    "idx = 0\n",
    "all_dialogues = [[\n",
    "    \"User: Alice moved to Munich.\",\n",
    "    \"Agent: That's interesting. What does she do there?\",\n",
    "    \"User: She works for Google.\",\n",
    "    \"Agent: And what is your relation to her?\",\n",
    "    \"User: She is my sister.\",\n",
    "],\n",
    "[\n",
    "   \"Speaker 1: Hey, you guys! Look what I found! Look at this!  That’s my Mom’s writing! Look.\",\n",
    "   \"Speaker 2: Me and Frank and Phoebe, Graduation 1965.\",\n",
    "   \"Speaker 1: Y'know what that means?\",\n",
    "   \"Speaker 3: That you’re actually 50?\",\n",
    "   \"Speaker 1: No-no, that’s not, that’s not me Phoebe, that’s her pal Phoebe. According to her high school yearbook, they were like B.F.F. Best Friends Forever.\",\n",
    "   \"Speaker 4: Oh!\",\n",
    "   \"Speaker 5: That is so cool.\",\n",
    "   \"Speaker 1: I know! So this woman probably could like have all kinds of stories about my parents, and she might even know like where my Dad is. So I looked her up, and she lives out by the beach. So maybe this weekend we could go to the beach?\",\n",
    "   \"Speaker 4: Yeah! Yeah, we can!\",\n",
    "   \"Speaker 6: Shoot! I can’t go, I have to work!\",\n",
    "   \"Speaker 7: That’s too bad.\",\n",
    "   \"Speaker 5: Ohh, big, fat bummerrr.\",\n",
    "   \"Speaker 1: So great! Okay! Tomorrow we’re gonna drive out to Montauk.\"\n",
    "  ]\n",
    "            ]\n",
    "\n",
    "dialogue = all_dialogues[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Alice', 'PERSON'), ('Munich', 'GPE')),\n",
       " (('Alice', 'PERSON'), ('Google', 'ORG')),\n",
       " (('Munich', 'GPE'), ('Alice', 'PERSON')),\n",
       " (('Munich', 'GPE'), ('Google', 'ORG')),\n",
       " (('Google', 'ORG'), ('Alice', 'PERSON')),\n",
       " (('Google', 'ORG'), ('Munich', 'GPE'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. extract SpaCy entities\n",
    "extractor = EntityExtractor()\n",
    "entity_pairs = extractor.process(' '.join(dialogue), ignore_types=['CARDINAL'])\n",
    "entity_pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['User: Alice moved to Munich.',\n",
       "   \"Agent: That's interesting. What does she do there?\",\n",
       "   'User: She works for Google.',\n",
       "   'Agent: And what is your relation to her?',\n",
       "   'User: She is my sister.'],\n",
       "  [{'x': 'Alice',\n",
       "    'x_type': 'PERSON',\n",
       "    'y': 'Munich',\n",
       "    'y_type': 'GPE',\n",
       "    'x_token_span': (2, 3),\n",
       "    'y_token_span': (5, 6),\n",
       "    'x_char_span': (6, 11),\n",
       "    'y_char_span': (21, 27),\n",
       "    'min_words_distance': 2,\n",
       "    'min_words_distance_pct': 0.011627906976744186,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'nsubj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 0,\n",
       "    'min_turn_distance_pct': 0.0},\n",
       "   {'x': 'Alice',\n",
       "    'x_type': 'PERSON',\n",
       "    'y': 'Google',\n",
       "    'y_type': 'ORG',\n",
       "    'x_token_span': (2, 3),\n",
       "    'y_token_span': (24, 25),\n",
       "    'x_char_span': (6, 11),\n",
       "    'y_char_span': (100, 106),\n",
       "    'min_words_distance': 21,\n",
       "    'min_words_distance_pct': 0.12209302325581395,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'nsubj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 2,\n",
       "    'min_turn_distance_pct': 0.011627906976744186},\n",
       "   {'x': 'Munich',\n",
       "    'x_type': 'GPE',\n",
       "    'y': 'Alice',\n",
       "    'y_type': 'PERSON',\n",
       "    'x_token_span': (2, 3),\n",
       "    'y_token_span': (5, 6),\n",
       "    'x_char_span': (6, 11),\n",
       "    'y_char_span': (21, 27),\n",
       "    'min_words_distance': 4,\n",
       "    'min_words_distance_pct': 0.023255813953488372,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'nsubj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 0,\n",
       "    'min_turn_distance_pct': 0.0},\n",
       "   {'x': 'Munich',\n",
       "    'x_type': 'GPE',\n",
       "    'y': 'Google',\n",
       "    'y_type': 'ORG',\n",
       "    'x_token_span': (5, 6),\n",
       "    'y_token_span': (24, 25),\n",
       "    'x_char_span': (21, 27),\n",
       "    'y_char_span': (100, 106),\n",
       "    'min_words_distance': 18,\n",
       "    'min_words_distance_pct': 0.10465116279069768,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'pobj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 2,\n",
       "    'min_turn_distance_pct': 0.011627906976744186},\n",
       "   {'x': 'Google',\n",
       "    'x_type': 'ORG',\n",
       "    'y': 'Alice',\n",
       "    'y_type': 'PERSON',\n",
       "    'x_token_span': (2, 3),\n",
       "    'y_token_span': (24, 25),\n",
       "    'x_char_span': (6, 11),\n",
       "    'y_char_span': (100, 106),\n",
       "    'min_words_distance': 23,\n",
       "    'min_words_distance_pct': 0.13372093023255813,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'nsubj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 2,\n",
       "    'min_turn_distance_pct': 0.011627906976744186},\n",
       "   {'x': 'Google',\n",
       "    'x_type': 'ORG',\n",
       "    'y': 'Munich',\n",
       "    'y_type': 'GPE',\n",
       "    'x_token_span': (5, 6),\n",
       "    'y_token_span': (24, 25),\n",
       "    'x_char_span': (21, 27),\n",
       "    'y_char_span': (100, 106),\n",
       "    'min_words_distance': 20,\n",
       "    'min_words_distance_pct': 0.11627906976744186,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'pobj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 2,\n",
       "    'min_turn_distance_pct': 0.011627906976744186}])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. enrich entity with features\n",
    "dialogues = [\n",
    "    (dialogue,\n",
    "     [{\n",
    "         'x': x, 'x_type': xt,\n",
    "         'y': y, 'y_type': yt,\n",
    "         } for ((x,xt),(y,yt))\n",
    "      in entity_pairs])\n",
    "]\n",
    "\n",
    "\n",
    "enricher = DialogueEnricher()\n",
    "\n",
    "enriched_dialogues = enricher.enrich(dialogues)\n",
    "enriched_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer whether entity has relation or not\n",
    "\n",
    "model_path = LOCAL_MODELS_PATH / 'custom/relation-identification/xgboost/dialog-re-binary-validated-enriched'\n",
    "\n",
    "def load_model(path):\n",
    "    model = pickle.load(open(os.path.join(path, 'model.pkl'), 'rb'))\n",
    "    le_dict = pickle.load(open(os.path.join(path, 'label_encoder_dict.pkl'), 'rb'))\n",
    "    vectorizer = pickle.load(open(os.path.join(path, 'vectorizer.pkl'), 'rb'))\n",
    "    scaler = pickle.load(open(os.path.join(path, 'scaler.pkl'), 'rb'))\n",
    "    return model, le_dict, vectorizer, scaler\n",
    "\n",
    "\n",
    "model, le_dict, vectorizer, scaler = load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and paste code from script \n",
    "# @todo: refactor into module\n",
    "import re\n",
    "import pandas as pd\n",
    "# set entity token markers\n",
    "ENTITY_X_TOKEN = 'x_marker'\n",
    "ENTITY_Y_TOKEN = 'y_marker'\n",
    "\n",
    "def mark_entities(df_relations):\n",
    "    df_relations['Dialogue'] = df_relations.apply(lambda row: \n",
    "                                                 [re.sub(r'\\b' + re.escape(row['x']) + r'\\b', ENTITY_X_TOKEN, \n",
    "                                                    re.sub(r'\\b' + re.escape(row['y']) + r'\\b', ENTITY_Y_TOKEN, sentence))\n",
    "                                                 for sentence in row['Dialogue']],\n",
    "                                                 axis=1)\n",
    "    return df_relations\n",
    "\n",
    "\n",
    "def feature_engineering(df_relations, mode='train', label_encoders=None, vectorizers=None):\n",
    "\n",
    "    le_dict = {} if label_encoders is None else label_encoders\n",
    "    for col in ['x_type', 'y_type']:\n",
    "        if mode == 'train':\n",
    "            le = LabelEncoder()\n",
    "            df_relations[col] = le.fit_transform(df_relations[col])\n",
    "            le_dict[col] = le\n",
    "        else:\n",
    "            df_relations[col] = le_dict[col].transform(df_relations[col])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        le = LabelEncoder()\n",
    "        df_relations['r'] = le.fit_transform(df_relations['r'])\n",
    "        le_dict['r'] = le\n",
    "\n",
    "    scaler = None\n",
    "    add_dialogue_as_features = True\n",
    "    vectorizer = vectorizers\n",
    "    if add_dialogue_as_features:\n",
    "        if mode == 'train':\n",
    "            vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "            TFIDF = vectorizer.fit_transform(df_relations['Dialogue'].apply(lambda x: ' '.join(x))).toarray()\n",
    "        else:\n",
    "            TFIDF = vectorizer.transform(df_relations['Dialogue'].apply(lambda x: ' '.join(x))).toarray()\n",
    "\n",
    "        tfidf_df = pd.DataFrame(TFIDF, columns=vectorizer.get_feature_names_out())\n",
    "        df_relations = pd.concat([df_relations.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    train_data = df_relations[df_relations['Origin'] == 'train']\n",
    "    test_data = df_relations[df_relations['Origin'] == 'test']\n",
    "    dev_data = df_relations[df_relations['Origin'] == 'dev']\n",
    "\n",
    "    drop_cols = ['x', 'y', 't', 'rid', \n",
    "                 'Origin', 'Dialogue', \n",
    "                 'x_token_span', 'y_token_span',\n",
    "                 'x_char_span', 'y_char_span',\n",
    "                 'min_words_distance_pct',\n",
    "                 'min_turn_distance_pct', \n",
    "                 'spacy_features.x_pos', 'spacy_features.x_dep',\n",
    "                 'spacy_features.x_tag', 'spacy_features.y_pos',\n",
    "                 'spacy_features.y_dep', 'spacy_features.y_tag'\n",
    "                 ]\n",
    "\n",
    "    if mode == 'infer':\n",
    "        drop_cols.append('r')\n",
    "\n",
    "    drop_cols = [col for col in drop_cols if col in train_data.columns]\n",
    "\n",
    "    X_train = train_data.drop(drop_cols, axis=1)\n",
    "    X_test = test_data.drop(drop_cols, axis=1)\n",
    "    X_dev = dev_data.drop(drop_cols, axis=1)\n",
    "\n",
    "    y_train = train_data['r'] if mode == 'train' else None\n",
    "    y_test = test_data['r'] if mode == 'train' else None\n",
    "    y_dev = dev_data['r'] if mode == 'train' else None\n",
    "\n",
    "    return X_train, X_test, X_dev, y_train, y_test, y_dev, vectorizer, le_dict, scaler\n",
    "\n",
    "\n",
    "def preprocess_data(df, mode='train'):\n",
    "        \n",
    "    spacy_entity_map = {\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"STRING\": \"PRODUCT\",  # Approximating common nouns to PRODUCT, @todo: use NOUN strategy.\n",
    "        \"GPE\": \"GPE\",\n",
    "        \"VALUE\": \"QUANTITY\",\n",
    "        \"ORG\": \"ORG\",\n",
    "        \"PERSON\": \"PERSON\",\n",
    "        \"PRODUCT\": \"PRODUCT\",\n",
    "        \"CARDINAL\": \"CARDINAL\",\n",
    "        \"DATE\": \"QUANTITY\"\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df_relations = df.explode('Relations').apply(lambda r: {**{\"Origin\": r['Origin'], 'Dialogue': r['Dialogue']}, **r['Relations']}, axis=1)\n",
    "    df_relations = pd.json_normalize(df_relations)\n",
    "\n",
    "    mask = df_relations.min_words_distance.isna()\n",
    "    df_relations = df_relations.dropna()\n",
    "\n",
    "    if mode == 'train':\n",
    "        df_relations['r'] = df_relations['r'].str[0]\n",
    "    df_relations['x_type'] = df_relations['x_type'].map(spacy_entity_map)\n",
    "    df_relations['y_type'] = df_relations['y_type'].map(spacy_entity_map)\n",
    "    df_relations = mark_entities(df_relations)\n",
    "    \n",
    "    return df_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(enriched_dialogues).rename({\n",
    "    0: 'Dialogue', 1: 'Relations'\n",
    "}, axis = 1)\n",
    "\n",
    "df['Origin'] = 'test'\n",
    "\n",
    "df = preprocess_data(df, mode='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_test, _, _, _, _, _, _, _ = feature_engineering(df, mode='infer', label_encoders=le_dict, vectorizers=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "threshold = 0.5\n",
    "D_test = xgb.DMatrix(X_test)\n",
    "preds = model.predict(D_test)\n",
    "pred_labels = np.where(preds > threshold, 1, 0)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T2 = 0.32\n",
    "relation_type_count = 36\n",
    "bert_config_file = LOCAL_MODELS_PATH / \"downloaded/bert-base/bert_config.json\"\n",
    "vocab_file = LOCAL_MODELS_PATH / \"downloaded/bert-base/vocab.txt\"\n",
    "model_path=LOCAL_MODELS_PATH / \"fine-tuned/bert-base-dialog-re/Unfrozen/24bs-1cls-3em5lr-20ep/model_best.pt\"\n",
    "relation_label_dict = LOCAL_RAW_DATA_PATH / 'dialog-re/relation_label_dict.json'\n",
    "\n",
    "inferer = EntityRelationInferer(\n",
    "    bert_config_file = bert_config_file, \n",
    "    vocab_file = vocab_file, \n",
    "    model_path = model_path, \n",
    "    relation_type_count = relation_type_count, \n",
    "    relation_label_dict = relation_label_dict,\n",
    "    T2 = T2)\n",
    "\n",
    "for i, r in enumerate(enriched_dialogues[0][1]):\n",
    "    r['r_bool'] = pred_labels[i]\n",
    "    r['t'] = \"[]\"\n",
    "    if  pred_labels[i] != 1:\n",
    "        continue\n",
    "    ent_x, ent_y = r['x'], r['y']\n",
    "    rid_prediction, relation_label = inferer.infer_relations(' '.join(dialogue), ent_x, ent_y)\n",
    "    r['rid'] = [rid_prediction]\n",
    "    r['r'] = [relation_label]\n",
    "\n",
    "predicted_relations = enriched_dialogues[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(r['x'], r['x_type'] )for r in predicted_relations if r['x'] == 'Phoebe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2569096/2326086674.py:16: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(graph._add_dataset, dataset_name)\n",
      "/tmp/ipykernel_2569096/2326086674.py:21: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(graph._add_dialogue_to_dataset, exporter.dialogue_id, dataset_name)\n"
     ]
    }
   ],
   "source": [
    "processor  = DialogueProcessor('pipeline')\n",
    "processor.process_dialogue(dialogue, predicted_relations)\n",
    "processor.close_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Alice moved to Munich.\n",
      "Agent: That's interesting. What does she do there?\n",
      "User: She works for Google.\n",
      "Agent: And what is your relation to her?\n",
      "User: She is my sister.\n"
     ]
    }
   ],
   "source": [
    "for t in all_dialogues[0]:\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
