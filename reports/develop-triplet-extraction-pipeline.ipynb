{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from src.paths import LOCAL_MODELS_PATH\n",
    "from src.processing.text_preprocessing import DialogueEnricher\n",
    "\n",
    "from src.infering import EntityRelationInferer\n",
    "from src.paths import LOCAL_MODELS_PATH, LOCAL_PROCESSED_DATA_PATH, LOCAL_RAW_DATA_PATH\n",
    "\n",
    "from src.infering import EntityExtractor\n",
    "\n",
    "dialogue = [\n",
    "    \"User: Alice moved to Munich.\",\n",
    "    \"Agent: That's interesting. What does she do there?\",\n",
    "    \"User: She works for Google.\",\n",
    "    \"Agent: And what is your relation to her?\",\n",
    "    \"User: She is my sister.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Alice', 'PERSON'), ('Munich', 'GPE')),\n",
       " (('Alice', 'PERSON'), ('Google', 'ORG')),\n",
       " (('Munich', 'GPE'), ('Alice', 'PERSON')),\n",
       " (('Munich', 'GPE'), ('Google', 'ORG')),\n",
       " (('Google', 'ORG'), ('Alice', 'PERSON')),\n",
       " (('Google', 'ORG'), ('Munich', 'GPE'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. extract SpaCy entities\n",
    "extractor = EntityExtractor()\n",
    "entity_pairs = extractor.process(' '.join(dialogue))\n",
    "entity_pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 1/1 [00:00<00:00, 69.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['User: Alice moved to Munich.',\n",
       "   \"Agent: That's interesting. What does she do there?\",\n",
       "   'User: She works for Google.',\n",
       "   'Agent: And what is your relation to her?',\n",
       "   'User: She is my sister.'],\n",
       "  [{'x': 'Alice',\n",
       "    'x_type': 'PERSON',\n",
       "    'y': 'Munich',\n",
       "    'y_type': 'GPE',\n",
       "    'x_token_span': (2, 3),\n",
       "    'y_token_span': (5, 6),\n",
       "    'x_char_span': (6, 11),\n",
       "    'y_char_span': (21, 27),\n",
       "    'min_words_distance': 2,\n",
       "    'min_words_distance_pct': 0.011627906976744186,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'nsubj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 0,\n",
       "    'min_turn_distance_pct': 0.0},\n",
       "   {'x': 'Alice',\n",
       "    'x_type': 'PERSON',\n",
       "    'y': 'Google',\n",
       "    'y_type': 'ORG',\n",
       "    'x_token_span': (2, 3),\n",
       "    'y_token_span': (24, 25),\n",
       "    'x_char_span': (6, 11),\n",
       "    'y_char_span': (100, 106),\n",
       "    'min_words_distance': 21,\n",
       "    'min_words_distance_pct': 0.12209302325581395,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'nsubj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 2,\n",
       "    'min_turn_distance_pct': 0.011627906976744186},\n",
       "   {'x': 'Munich',\n",
       "    'x_type': 'GPE',\n",
       "    'y': 'Alice',\n",
       "    'y_type': 'PERSON',\n",
       "    'x_token_span': (2, 3),\n",
       "    'y_token_span': (5, 6),\n",
       "    'x_char_span': (6, 11),\n",
       "    'y_char_span': (21, 27),\n",
       "    'min_words_distance': 4,\n",
       "    'min_words_distance_pct': 0.023255813953488372,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'nsubj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 0,\n",
       "    'min_turn_distance_pct': 0.0},\n",
       "   {'x': 'Munich',\n",
       "    'x_type': 'GPE',\n",
       "    'y': 'Google',\n",
       "    'y_type': 'ORG',\n",
       "    'x_token_span': (5, 6),\n",
       "    'y_token_span': (24, 25),\n",
       "    'x_char_span': (21, 27),\n",
       "    'y_char_span': (100, 106),\n",
       "    'min_words_distance': 18,\n",
       "    'min_words_distance_pct': 0.10465116279069768,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'pobj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 2,\n",
       "    'min_turn_distance_pct': 0.011627906976744186},\n",
       "   {'x': 'Google',\n",
       "    'x_type': 'ORG',\n",
       "    'y': 'Alice',\n",
       "    'y_type': 'PERSON',\n",
       "    'x_token_span': (2, 3),\n",
       "    'y_token_span': (24, 25),\n",
       "    'x_char_span': (6, 11),\n",
       "    'y_char_span': (100, 106),\n",
       "    'min_words_distance': 23,\n",
       "    'min_words_distance_pct': 0.13372093023255813,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'nsubj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 2,\n",
       "    'min_turn_distance_pct': 0.011627906976744186},\n",
       "   {'x': 'Google',\n",
       "    'x_type': 'ORG',\n",
       "    'y': 'Munich',\n",
       "    'y_type': 'GPE',\n",
       "    'x_token_span': (5, 6),\n",
       "    'y_token_span': (24, 25),\n",
       "    'x_char_span': (21, 27),\n",
       "    'y_char_span': (100, 106),\n",
       "    'min_words_distance': 20,\n",
       "    'min_words_distance_pct': 0.11627906976744186,\n",
       "    'spacy_features': {'x_pos': 'PROPN',\n",
       "     'x_dep': 'pobj',\n",
       "     'x_tag': 'NNP',\n",
       "     'y_pos': 'PROPN',\n",
       "     'y_dep': 'pobj',\n",
       "     'y_tag': 'NNP'},\n",
       "    'min_turn_distance': 2,\n",
       "    'min_turn_distance_pct': 0.011627906976744186}])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. enrich entity with features\n",
    "dialogues = [\n",
    "    (dialogue,\n",
    "     [{\n",
    "         'x': x, 'x_type': xt,\n",
    "         'y': y, 'y_type': yt,\n",
    "         } for ((x,xt),(y,yt))\n",
    "      in entity_pairs])\n",
    "]\n",
    "\n",
    "\n",
    "enricher = DialogueEnricher()\n",
    "\n",
    "enriched_dialogues = enricher.enrich(dialogues)\n",
    "enriched_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer whether entity has relation or not\n",
    "\n",
    "model_path = LOCAL_MODELS_PATH / 'custom/relation-identification/xgboost/dialog-re-binary-enriched-local'\n",
    "\n",
    "def load_model(path):\n",
    "    model = pickle.load(open(os.path.join(path, 'model.pkl'), 'rb'))\n",
    "    le_dict = pickle.load(open(os.path.join(path, 'label_encoder_dict.pkl'), 'rb'))\n",
    "    vectorizer = pickle.load(open(os.path.join(path, 'vectorizer.pkl'), 'rb'))\n",
    "    scaler = pickle.load(open(os.path.join(path, 'scaler.pkl'), 'rb'))\n",
    "    return model, le_dict, vectorizer, scaler\n",
    "\n",
    "\n",
    "model, le_dict, vectorizer, scaler = load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and paste code from script \n",
    "# @todo: refactor into module\n",
    "import re\n",
    "# set entity token markers\n",
    "ENTITY_X_TOKEN = 'x_marker'\n",
    "ENTITY_Y_TOKEN = 'y_marker'\n",
    "\n",
    "def mark_entities(df_relations):\n",
    "    df_relations['Dialogue'] = df_relations.apply(lambda row: \n",
    "                                                 [re.sub(r'\\b' + re.escape(row['x']) + r'\\b', ENTITY_X_TOKEN, \n",
    "                                                    re.sub(r'\\b' + re.escape(row['y']) + r'\\b', ENTITY_Y_TOKEN, sentence))\n",
    "                                                 for sentence in row['Dialogue']],\n",
    "                                                 axis=1)\n",
    "    return df_relations\n",
    "\n",
    "\n",
    "def feature_engineering(df_relations, mode='train', label_encoders=None, vectorizers=None):\n",
    "\n",
    "    le_dict = {} if label_encoders is None else label_encoders\n",
    "    for col in ['x_type', 'y_type']:\n",
    "        if mode == 'train':\n",
    "            le = LabelEncoder()\n",
    "            df_relations[col] = le.fit_transform(df_relations[col])\n",
    "            le_dict[col] = le\n",
    "        else:\n",
    "            df_relations[col] = le_dict[col].transform(df_relations[col])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        le = LabelEncoder()\n",
    "        df_relations['r'] = le.fit_transform(df_relations['r'])\n",
    "        le_dict['r'] = le\n",
    "\n",
    "    scaler = None\n",
    "    add_dialogue_as_features = True\n",
    "    vectorizer = vectorizers\n",
    "    if add_dialogue_as_features:\n",
    "        if mode == 'train':\n",
    "            vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "            TFIDF = vectorizer.fit_transform(df_relations['Dialogue'].apply(lambda x: ' '.join(x))).toarray()\n",
    "        else:\n",
    "            TFIDF = vectorizer.transform(df_relations['Dialogue'].apply(lambda x: ' '.join(x))).toarray()\n",
    "\n",
    "        tfidf_df = pd.DataFrame(TFIDF, columns=vectorizer.get_feature_names_out())\n",
    "        df_relations = pd.concat([df_relations.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    train_data = df_relations[df_relations['Origin'] == 'train']\n",
    "    test_data = df_relations[df_relations['Origin'] == 'test']\n",
    "    dev_data = df_relations[df_relations['Origin'] == 'dev']\n",
    "\n",
    "    drop_cols = ['x', 'y', 't', 'rid', \n",
    "                 'Origin', 'Dialogue', \n",
    "                 'x_token_span', 'y_token_span',\n",
    "                 'x_char_span', 'y_char_span',\n",
    "                 'min_words_distance_pct',\n",
    "                 'min_turn_distance_pct', \n",
    "                 'spacy_features.x_pos', 'spacy_features.x_dep',\n",
    "                 'spacy_features.x_tag', 'spacy_features.y_pos',\n",
    "                 'spacy_features.y_dep', 'spacy_features.y_tag'\n",
    "                 ]\n",
    "\n",
    "    if mode == 'infer':\n",
    "        drop_cols.append('r')\n",
    "\n",
    "    drop_cols = [col for col in drop_cols if col in train_data.columns]\n",
    "\n",
    "    X_train = train_data.drop(drop_cols, axis=1)\n",
    "    X_test = test_data.drop(drop_cols, axis=1)\n",
    "    X_dev = dev_data.drop(drop_cols, axis=1)\n",
    "\n",
    "    y_train = train_data['r'] if mode == 'train' else None\n",
    "    y_test = test_data['r'] if mode == 'train' else None\n",
    "    y_dev = dev_data['r'] if mode == 'train' else None\n",
    "\n",
    "    return X_train, X_test, X_dev, y_train, y_test, y_dev, vectorizer, le_dict, scaler\n",
    "\n",
    "def preprocess_data(df):\n",
    "        \n",
    "    spacy_entity_map = {\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"STRING\": \"PRODUCT\",  # Approximating common nouns to PRODUCT, @todo: use NOUN strategy.\n",
    "        \"GPE\": \"GPE\",\n",
    "        \"VALUE\": \"QUANTITY\",\n",
    "        \"ORG\": \"ORG\",\n",
    "    }\n",
    "    \n",
    "    df_relations = df.explode('Relations').apply(lambda r: {**{\"Origin\": r['Origin'], 'Dialogue': r['Dialogue']}, **r['Relations']}, axis=1)\n",
    "    df_relations = pd.json_normalize(df_relations)\n",
    "\n",
    "    mask = df_relations.min_words_distance.isna()\n",
    "    df_relations = df_relations.dropna()\n",
    "    df_relations['r'] = df_relations['r'].str[0]\n",
    "\n",
    "    df_relations['x_type'] = df_relations['x_type'].map(spacy_entity_map)\n",
    "    df_relations['y_type'] = df_relations['y_type'].map(spacy_entity_map)\n",
    "    df_relations = mark_entities(df_relations)\n",
    "    \n",
    "    return df_relations\n",
    "\n",
    "def preprocess_data(df, mode='train'):\n",
    "        \n",
    "    spacy_entity_map = {\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"STRING\": \"PRODUCT\",  # Approximating common nouns to PRODUCT, @todo: use NOUN strategy.\n",
    "        \"GPE\": \"GPE\",\n",
    "        \"VALUE\": \"QUANTITY\",\n",
    "        \"ORG\": \"ORG\",\n",
    "    }\n",
    "    \n",
    "    df_relations = df.explode('Relations').apply(lambda r: {**{\"Origin\": r['Origin'], 'Dialogue': r['Dialogue']}, **r['Relations']}, axis=1)\n",
    "    df_relations = pd.json_normalize(df_relations)\n",
    "\n",
    "    mask = df_relations.min_words_distance.isna()\n",
    "    df_relations = df_relations.dropna()\n",
    "\n",
    "    if mode == 'train':\n",
    "        df_relations['r'] = df_relations['r'].str[0]\n",
    "        df_relations['x_type'] = df_relations['x_type'].map(spacy_entity_map)\n",
    "        df_relations['y_type'] = df_relations['y_type'].map(spacy_entity_map)\n",
    "    df_relations = mark_entities(df_relations)\n",
    "    \n",
    "    return df_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dialogues).rename({\n",
    "    0: 'Dialogue', 1: 'Relations'\n",
    "}, axis = 1)\n",
    "\n",
    "df['Origin'] = 'test'\n",
    "\n",
    "df = preprocess_data(df, mode='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_test, _, _, _, _, _, _, _ = feature_engineering(df, mode='infer', label_encoders=le_dict, vectorizers=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_type</th>\n",
       "      <th>y_type</th>\n",
       "      <th>min_words_distance</th>\n",
       "      <th>min_turn_distance</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>009</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zillionaire</th>\n",
       "      <th>zinfandel</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>ztez</th>\n",
       "      <th>zxy</th>\n",
       "      <th>zzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 9192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_type  y_type  min_words_distance  min_turn_distance   00  000  007  009  \\\n",
       "0       2       0                   2                  0  0.0  0.0  0.0  0.0   \n",
       "1       2       1                  21                  2  0.0  0.0  0.0  0.0   \n",
       "2       0       2                   4                  0  0.0  0.0  0.0  0.0   \n",
       "3       0       1                  18                  2  0.0  0.0  0.0  0.0   \n",
       "4       1       2                  23                  2  0.0  0.0  0.0  0.0   \n",
       "5       1       0                  20                  2  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    10  100  ...  zillion  zillionaire  zinfandel  zip  zone  zoo  zoom  ztez  \\\n",
       "0  0.0  0.0  ...      0.0          0.0        0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "1  0.0  0.0  ...      0.0          0.0        0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "2  0.0  0.0  ...      0.0          0.0        0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "3  0.0  0.0  ...      0.0          0.0        0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "4  0.0  0.0  ...      0.0          0.0        0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "5  0.0  0.0  ...      0.0          0.0        0.0  0.0   0.0  0.0   0.0   0.0   \n",
       "\n",
       "   zxy  zzz  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "5  0.0  0.0  \n",
       "\n",
       "[6 rows x 9192 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "D_test = xgb.DMatrix(X_test)\n",
    "preds = model.predict(D_test)\n",
    "pred_labels = np.where(preds > 0.6, 1, 0)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T2 = 0.32\n",
    "relation_type_count = 36\n",
    "bert_config_file = LOCAL_MODELS_PATH / \"downloaded/bert-base/bert_config.json\"\n",
    "vocab_file = LOCAL_MODELS_PATH / \"downloaded/bert-base/vocab.txt\"\n",
    "model_path=LOCAL_MODELS_PATH / \"fine-tuned/bert-base-dialog-re/Unfrozen/24bs-1cls-3em5lr-20ep/model_best.pt\"\n",
    "relation_label_dict = LOCAL_RAW_DATA_PATH / 'dialog-re/relation_label_dict.json'\n",
    "\n",
    "inferer = EntityRelationInferer(\n",
    "    bert_config_file = bert_config_file, \n",
    "    vocab_file = vocab_file, \n",
    "    model_path = model_path, \n",
    "    relation_type_count = relation_type_count, \n",
    "    relation_label_dict = relation_label_dict,\n",
    "    T2 = T2)\n",
    "\n",
    "rid_prediction, relation_label = inferer.infer_relations(' '.join(dialogue_list), ent_x, ent_y)\n",
    "\n",
    "\n",
    "\n",
    "le_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
